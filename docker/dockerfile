# Use an official OpenJDK base image
FROM azul/zulu-openjdk-debian:17-jre-latest

# Set default environment variables
ENV SPARK_VERSION=4.0.0 \
    HADOOP_VERSION=3 \
    SPARK_HOME=/opt/spark \
    SPARK_ROLE=master \
    SPARK_MASTER_HOST=localhost \
    SPARK_MASTER_PORT=7077 \
    SPARK_WORKER_CORES=2 \
    SPARK_WORKER_MEMORY=2g \
    SPARK_HISTORY_OPTS=""

# Install dependencies
RUN apt-get update && apt-get install -y curl bash procps && \
    rm -rf /var/lib/apt/lists/*

# Download and extract Spark
RUN curl -fsSL https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    | tar -xz -C /opt && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME}

RUN apt-get update && apt-get install -y --no-install-recommends \
iputils-ping \
netcat-openbsd \
&& apt-get clean \
&& rm -rf /var/lib/apt/lists/*

# Set PATH
ENV PATH="${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}"

# Create a work directory
WORKDIR /opt/workspace

# Copy entrypoint script
COPY docker-entrypoint.sh /opt/
RUN chmod +x /opt/docker-entrypoint.sh

ENTRYPOINT ["/opt/docker-entrypoint.sh"]

# Default command (overridable)
CMD ["spark-shell"]
